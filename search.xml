<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>boost</title>
      <link href="/2023/07/29/boost/"/>
      <url>/2023/07/29/boost/</url>
      
        <content type="html"><![CDATA[<h1 id="梯度增强笔记">梯度增强笔记</h1><h2 id="集成模型">1 集成模型</h2><p>集成模型（EnsembleModel）不是一种具体的模型，而是一种模型框架，将若干个模型(弱学习器)按照一定的策略组合起来，共同完成一个任务。</p><h3 id="bagging模型">1.1 bagging模型</h3><p>bagging模型会把(相同的)若干基础模型简单的“装起来”——基础模型独立训练，然后将它们的输出用特定的规则综合(比如求平均值)起来，形成最后的预测值。在回归任务中，通常认为基础模型分布在真实值周围，将输出平均一下，即得到一个稳定的预测值。（随机森林）<imgsrc="https://pic3.zhimg.com/80/v2-4b31a06b0f7168503443e67d98cfde4e_720w.webp"alt="bagging" /></p><h3 id="stacking模型">1.2 stacking模型</h3><p>stacking模型是bagging模型的增强版，允许使用不同的基础模型通过不同权重进行决策。<imgsrc="https://pic4.zhimg.com/80/v2-60109736ab4331374e4307a625d67c7b_720w.webp"alt="stacking" /></p><h3 id="boosting模型">1.3 boosting模型</h3><p>boosting模型是通过串联形式将基础模型联系起来，每一级基础模型会对前级模型的残差，即弱学习器不完美的部分补全，最终不断缩小残差，达到优良的拟合效果。<imgsrc="https://pic3.zhimg.com/80/v2-b8efd3f521896f4387ca49c03fdc94f6_720w.webp"alt="boosting" /></p><h2 id="回归任务的梯度增强">2 回归任务的梯度增强</h2><h3 id="简介">2.1 简介</h3><p>以数据集<span class="math inline">\({\rm{D} } =[({X_1},{y_1}),({X_2},{y_2}),...,({X_n},{y_n})]\)</span>，为例。拟合<spanclass="math inline">\({y_n} = f({X_n})\)</span>,降低误差，实现预测。设第<span class="math inline">\(k\)</span>级的决策树为<spanclass="math inline">\({T_k}({X_n})\)</span>，这一级的决策树是弥补上一级决策树的残差，即训练：<spanclass="math inline">\({\hat \varepsilon _{k - 1} } = {T_k}({x_n}) +{\varepsilon _k}\)</span>。根据该递归公式求和，最后一级残差忽略不计，可知，最终的预测模型为：${y_n}= _{i = 1}^k { {T_i}({x_n})} $。</p><h3 id="训练方法">2.2 训练方法</h3><p>在训练第k个决策树的时候，我们需要最小化这样一个loss函数： <spanclass="math inline">\(J = \sum \limits _{i = 1}^n {L({y_i},{f_k}({x_i} )) }\)</span>，而<span class="math inline">\({f_k}({x_i}) = {f_{k - 1}}({x_i}) + {T_k}({x_i})\)</span>，通过更新<spanclass="math inline">\({f_k}({x_i})\)</span>，使损失函数最小化。即每次学习：<span class="math inline">\({f_k}({x_i}) = {f_{k - 1}}({x_i}) - \alpha \times \frac{ {\partial J} } { {\partial {f_{k - 1} }} }\)</span>(<span class="math inline">\(\alpha\)</span>为学习率)。得到<span class="math inline">\(k\)</span>级决策树：<spanclass="math inline">\({T_k}({x_n}) = - \alpha \times \frac{ {\partial J}} { {\partial {f_{k - 1} } } }\)</span>。 采用残差平方和<spanclass="math inline">\(J = \sum\limits_{i = 1}^N {\frac{1}{2} { {({y_i} -{f_k}({x_i}))}^2} }\)</span>作为损失函数，可以得到第<spanclass="math inline">\(k\)</span>级决策树是拟合前级决策树的残差。得到：<spanclass="math inline">\({T_k}({x_n}) = - \alpha \times \frac{ {\partial J}}{ {\partial {f_{k - 1} }} } = \alpha ({y_n} - {f_{k - 1}}({x_n}))\)</span>。</p><h2 id="梯度增强实现">3 梯度增强实现</h2><h3 id="训练">3.1 训练</h3><p>在训练每一个决策树时，需要逐级训练，训练速度较慢。</p><h3 id="预测">3.2 预测</h3><p>预测时，每个决策树可以单独计算，可以通过并行计算提升速度。</p><h2 id="示例代码">4 示例代码</h2><p>摘自<ahref="https://github.com/lipengyuer/DataScience/blob/master/src/algoritm/GBDTRegression.py">GBDTRegression</a>。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2023/07/28/root/"/>
      <url>/2023/07/28/root/</url>
      
        <content type="html"><![CDATA[<h1 id="ɢģ">��ɢ����ģ��</h1><h2 id="ʶ">1 ����֪ʶ</h2><h3 id="section">1.0 ����</h3><p><a href="https://arxiv.org/abs/2006.11239">Denoising DiffusionProbabilistic Models</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/07/28/hello-world/"/>
      <url>/2023/07/28/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><div class="sourceCode" id="cb1"><preclass="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> hexo new <span class="st">&quot;My New Post&quot;</span></span></code></pre></div><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><div class="sourceCode" id="cb2"><preclass="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> hexo server</span></code></pre></div><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><div class="sourceCode" id="cb3"><preclass="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> hexo generate</span></code></pre></div><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><div class="sourceCode" id="cb4"><preclass="sourceCode bash"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> hexo deploy</span></code></pre></div><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
