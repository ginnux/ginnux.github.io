<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>boost</title>
      <link href="/2023/07/29/boost/"/>
      <url>/2023/07/29/boost/</url>
      
        <content type="html"><![CDATA[<h1 id="梯度增强笔记"><a href="#梯度增强笔记" class="headerlink" title="梯度增强笔记"></a>梯度增强笔记</h1><h2 id="1-集成模型"><a href="#1-集成模型" class="headerlink" title="1 集成模型"></a>1 集成模型</h2><p>集成模型（Ensemble Model）不是一种具体的模型，而是一种模型框架，将若干个模型(弱学习器)按照一定的策略组合起来，共同完成一个任务。</p><h3 id="1-1-bagging模型"><a href="#1-1-bagging模型" class="headerlink" title="1.1 bagging模型"></a>1.1 bagging模型</h3><p>bagging模型会把(相同的)若干基础模型简单的“装起来”——基础模型独立训练，然后将它们的输出用特定的规则综合(比如求平均值)起来，形成最后的预测值。在回归任务中，通常认为基础模型分布在真实值周围，将输出平均一下，即得到一个稳定的预测值。（随机森林）<br><img src="https://pic3.zhimg.com/80/v2-4b31a06b0f7168503443e67d98cfde4e_720w.webp" alt="bagging"></p><h3 id="1-2-stacking模型"><a href="#1-2-stacking模型" class="headerlink" title="1.2 stacking模型"></a>1.2 stacking模型</h3><p>stacking模型是bagging模型的增强版，允许使用不同的基础模型通过不同权重进行决策。<br><img src="https://pic4.zhimg.com/80/v2-60109736ab4331374e4307a625d67c7b_720w.webp" alt="stacking"></p><h3 id="1-3-boosting模型"><a href="#1-3-boosting模型" class="headerlink" title="1.3 boosting模型"></a>1.3 boosting模型</h3><p>boosting模型是通过串联形式将基础模型联系起来，每一级基础模型会对前级模型的残差，即弱学习器不完美的部分补全，最终不断缩小残差，达到优良的拟合效果。<br><img src="https://pic3.zhimg.com/80/v2-b8efd3f521896f4387ca49c03fdc94f6_720w.webp" alt="boosting"></p><h2 id="2-回归任务的梯度增强"><a href="#2-回归任务的梯度增强" class="headerlink" title="2 回归任务的梯度增强"></a>2 回归任务的梯度增强</h2><h3 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h3><p>以数据集<script type="math/tex">{\rm{D} } = [({X_1},{y_1}),({X_2},{y_2}),...,({X_n},{y_n})]</script>，为例。拟合${y_n} = f({X_n})$,降低误差，实现预测。<br>设第$k$级的决策树为${T_k}({X_n})$，这一级的决策树是弥补上一级决策树的残差，即训练：${\hat \varepsilon _{k - 1} } = {T_k}({x_n}) + {\varepsilon _k}$。<br>根据该递归公式求和，最后一级残差忽略不计，可知，最终的预测模型为：${\hat y_n} = \sum\limits_{i = 1}^k { {T_i}({x_n})} $。</p><h3 id="2-2-训练方法"><a href="#2-2-训练方法" class="headerlink" title="2.2 训练方法"></a>2.2 训练方法</h3><p>在训练第k个决策树的时候，我们需要最小化这样一个loss函数：<br>$J = \sum\limits_{i = 1}^n {L({y_i},{f_k}({x_i}))} $，而${f_k}({x_i}) = {f_{k - 1} }({x_i}) + {T_k}({x_i})$，通过更新${f_k}({x_i})$，使损失函数最小化。<br>即每次学习：${f_k}({x_i}) = {f_{k - 1} }({x_i}) - \alpha  \times \frac{ {\partial J} } { {\partial {f_{k - 1} } } }$($\alpha$为学习率)。<br>得到$k$级决策树：${T_k}({x_n}) =  - \alpha  \times \frac{ {\partial J} } { {\partial {f_{k - 1} } } }$。<br>采用残差平方和$J = \sum\limits_{i = 1}^N {\frac{1}{2} { {({y_i} - {f_k}({x_i}))}^2} } $作为损失函数，可以得到第$k$级决策树是拟合前级决策树的残差。得到：${T_k}({x_n}) =  - \alpha  \times \frac{ {\partial J} }{ {\partial {f_{k - 1} }} } = \alpha ({y_n} - {f_{k - 1} }({x_n}))$。</p><h2 id="3-梯度增强实现"><a href="#3-梯度增强实现" class="headerlink" title="3 梯度增强实现"></a>3 梯度增强实现</h2><h3 id="3-1-训练"><a href="#3-1-训练" class="headerlink" title="3.1 训练"></a>3.1 训练</h3><p>在训练每一个决策树时，需要逐级训练，训练速度较慢。</p><h3 id="3-2-预测"><a href="#3-2-预测" class="headerlink" title="3.2 预测"></a>3.2 预测</h3><p>预测时，每个决策树可以单独计算，可以通过并行计算提升速度。</p><h2 id="4-示例代码"><a href="#4-示例代码" class="headerlink" title="4 示例代码"></a>4 示例代码</h2><p>摘自<a href="https://github.com/lipengyuer/DataScience/blob/master/src/algoritm/GBDTRegression.py">GBDTRegression</a>。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2023/07/28/root/"/>
      <url>/2023/07/28/root/</url>
      
        <content type="html"><![CDATA[<h1 id="��ɢ����g��"><a href="#��ɢ����g��" class="headerlink" title="��ɢ����ģ��"></a>��ɢ����ģ��</h1><h2 id="1-����֪ʶ"><a href="#1-����֪ʶ" class="headerlink" title="1 ����֪ʶ"></a>1 ����֪ʶ</h2><h3 id="1-0-����"><a href="#1-0-����" class="headerlink" title="1.0 ����"></a>1.0 ����</h3><p><a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/07/28/hello-world/"/>
      <url>/2023/07/28/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="lang-bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="lang-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="lang-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="lang-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
